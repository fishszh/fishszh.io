---
title: 线性模型
keywords: [机器学习, 线性模型]
weight: 20
markup: mmark
toc: true  # Show table of contents? true/false
type: docs  # Do not modify.
menu:
    learning-machineLearning:
---
---
## 基本形式

给定由d个属性描述的示例$x=(x_1;x_2;\ldots;x_d)$,线性模型(linear model)试图学得一个
通过属性的线性组合来进行预测函数,即

$$f(x) = w_1 x_1 + w_2 x_2 + \ldots + w_d x_d + b$$

一般用向量形式写成

$$f(x) = w^T x + b$$

线性模型形式简单,易于建模,但却蕴含着机器学习中一些重要的基本思想.许多功能更为强大的非线性
模型(nonlinear model) 可在线性模型的基础上通过引入层级结构或高维映射而得.


## 线性回归

线性回归(linear regression)试图学得一个线性模型以尽可能准确地预测实值输出标记.

线性回归试图学得:

$$
f(x_i) = w_i x_i + b, 使得 f(x_i)\simeq y_i
$$

如何确定w和b呢? 均方差最小化,即

$$\begin{align}
(w^{*}, b^{*}) &= arg min \sum_{i=1}^{m}(f(x_i)-y_i)^{2} \\
&= arg min \sum_{i=1}^{m}(y_i-wx_i-b)^{2}
\end{align}$$

均方误差有非常好的集合意义,他对应于常用的欧几里得距离.基于均方误差最小来进行模型求解的方法
称为 **最小二乘法**(least square method).

在求解多远线性回归问题的时候,往往不满足满秩矩阵,这个时候可能会存在多个解的情况,选择哪一个
解作为输出,将由学习算法的归纳偏好决定,常见的做法是引入正则化(regularization)项.

## 对数几率回归

若果要处理的是分类问题,如二分类问题.引入 **单位阶跃函数**(unit-step function)

$$
z =
\begin{cases}
0, & z\lt 0  \\
0.5, & z = 0  \\
1,  & z>0
\end{cases}
$$

sigmoid函数:对数几率函数(logistic function):

$$
y = \frac{1}{1+e^{-z}}
$$

## 线性判别分析

线性判别分析(Linear Discriminant Analysis, LDA)是一种经典的线性学习方法,其思想非常朴素:
给定训练样例集,设法将样例投影到一条直线上,使得同类样例的投影点尽可能接近,一类样例的投影点
尽可能远离,在对新样本进行分析时,将其投影到同样的这条线上,再根据投影点的位置来确定新样本的
类别.

## 多分类学习

先对问题进行拆分,然后拆分出的每个二分类任务训练一个分类器;在测试时,对这些分类器的预测结果
进行集成以获得最终的多分类结果.这里的关键是如何进行拆分,以及如何对多个分类器进行集成.

最经典的拆分策略有三种:`一对一`,`一对其余`和`多对多`

<font color=blue>这里有一个关于纠错输出码(Error Correcting Output Codes)的介绍,有时间再研究
</font> ## TODO: Error Correcting Output Codes
