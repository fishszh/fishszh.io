---
title: 决策树
keywords: [机器学习, 决策树]
date: 2019-09-23
weight: 30
markup: mmark
toc: true  # Show table of contents? true/false
type: docs  # Do not modify.
menu:
    learning-machineLearning:
---
---
## 基本流程

决策树(decision tree) 是一类常见的机器学习方法.以二分类任务为例,我们希望从给定训练数据
集学得一个模型用于对新示例进行分类,这个把样本分类的任务,可以看做对"当前样本属于正类吗?"
这个问题的决策或者判定过程.顾名思义,决策树是基于树结构来进行决策的,这恰是人类在面临决策问题
时一种很自然的处理机制.

决策树学习的目的是为了产生一颗泛化能力强,即处理未见示例能力强的决策树,其基本流程遵循简单
且直观的"分而治之"策略.

**输入**:  
训练集: $D={(x_1,y_1),(x_2,y_2),...,(x_n,y_n)}$;  
属性值: $A = {a_1,a_2,...,a_n}$

**过程**: 函数TreeGenerate(D,A)

```
生成节点 node  
if D 中样本全属于同一类别 C then
    将node标记为 C类 叶节点; return
end if
if A = $\varnothing$ or D 中样本在A上取值相同 then
    将node标记为叶节点,其类别标记为 D 中样本数最多的类; return
end if
从A中选择最优化分属性 a*;
for a* 的没一个值 $a_v$ do
    为node生成一个分支;令 $D_v$ 表示D中在a*上取值为$a_v$的样本子集;
    if $D_v$ 为空 then
        将分支节点标记为叶节点,其标记类型为D中样本最多的类; return
    else
        以TreeGenerate($D_v$, A\{a*})为分支节点
    end if
end for
```

**输出**: 以node为根节点的一棵决策树

显然决策树的生成过是一个递归过程.在决策树基本算法中,有三种情形会导致递归返回:  

- 当前节点包含的样本原属于同一类别,无需划分  
- 当前属性集为空,或是所有样本在所有属性上取值相同,无法划分  
- 当前节点包含的样本集为空,不能划分

在第二种情况下,我们把当前节点标记为叶节点,并将其类别设定为该节点所含样本最多的类别;  
在第三种情况下, 同样把当前节点标记为叶节点,但将其类别设定为其父节点所含样本最多的类别.  

> 注意这两种情况的处理实质不同,情形二是在利用当前节点的后验分布,而情形三则是把父节点的
> 样本分布作为当前节点的先验分布

## 划分选择

决策树学习的关键是,如何选择最优划分属性.一般而言,随着划分过程不断进行,我们希望决策树的分支
节点所包含的样本尽可能属于同一类别,即节点的'纯度'越来越高.

### 信息增益

**信息熵**(information entropy) 是度量样本集合纯度最常用的一种指标.假定当前样本集合D中
第k类样本所占比例为$p_k$ (k=1,2,...),则D的信息熵定义为

$$Ent(D) = -\sum_{k} p_{k}log_{2}p_{k}$$

$Ent(D)$ 的值越小,则$D$的纯度越高.

**信息增益**:假定离散属性a 有 V 个可能的取值,若使用a来对样本集D进行划分,
则会产生V个分支节点.

$$
Gain(D,a) = Ent(D) - \sum_{v}\frac{|D_{v}|}{|D|}Ent(D_{v})
$$

一般而言,信息增益越大,则意味着使用属性a来划分所获得的纯度提升越大.因此,我们可用信息熵增
益来进行决策树的划分属性选择.

### 增益率

实际上,信息增益准则对可取值数目较多的属性有所偏好,为减少这种偏好可能带来的影响,著名
C4.5 决策树算法[Quinlan, 1993]不直接使用信息增益,而是使用`"增益率"`(gain ratio)来选择
最优划分属性,增益率定义为

$$Gai\_ration(D,a) = \frac{Gain(D,a)}{IV(a)}$$

其中

$$
IV(a) = - \sum_{v=1}^{V} \frac{|D_{v}|}{|D|} log_{2} \frac{|D_{v}|}{|D|}
$$

称为属性a的`固有值`(intrinsic value).属性a的可能取值数目越多(即V越大),则`IV(a)`的值通常会
越大.

> 需要注意的是,增益率准则对可取数目较少的属性有所偏好,因此,C4.5算法并不是直接选择
> 增益率最的候选划分属性,而是使用了一个启发:先从候选划分属性中找出信息增益高于平均
> 水平的属性,再从中选择增益率最高的.

### 基尼指数

CART 决策树[Breiman et al., 1984]使用`基尼指数`(Gini Index)来选择划分属性.
数据集D的纯度可用 **基尼值** 来度量:

$$
\begin{align}
Gini(D) &= \sum_{k=1}^{|y|}\sum_{k`\neq k} p_{k}p_{k`} \\
&= 1 - \sum_{k=1}^{|y|} p_{k}^{2}
\end{align}$$


只管来说,`Gini(D)`反应了数据集`D`中随机抽取两个样本,其类别标记不一致的概率.
因此,`Gini(D)`越小,则数据集`D`的纯度越高.

基尼指数定义为:

$$Gini\_index(D,a) = \sum_{v=1}^{V} \frac{|D_{v}|}{|D|} Gini(D_{v})$$

于是,我们在候选属性集合`A`中,选择那个使得划分后基尼指数最小的属性作为最优划分属性.

## 剪枝处理

**剪枝**pruning是决策树学习算法对付过拟合的主要手段.在决策树学习中,为了尽可能正确的分类训练样本,节点划分过程将不断重复,有时会造成决策树分支过多,这时就可能因训练样本学得太好了,以至于把训练集自身的一些特点当作所有数据都具有的一般性质而导致过拟合.因此,可通过主动去掉一些分支来降低过拟合风险.

决策树剪枝的基本策略有 **预剪枝**(prepruning)和 **后剪枝**(post-pruning).预剪枝是指在决策树生成过程中,对每个节点在划分前进行估计,若当前节点的划分不能带来决策树泛化性能提升,则停止划分并将当前节点标记为叶节点;后剪枝则是先从训练集生成一颗完整的决策树,然后自底向上的对非叶节点进行考察,若将该节点对应的子树替换为叶节点能带来决策树泛化性能的提升,则将该子树替换为叶节点.

## 缺失值处理

可能面临的问题:

1. 如何在属性值确实的情况下进行划分属性选择
2. 给定划分属性,若样本在该属性上的值缺失,如何对样本进行划分.


## 多变量决策树
